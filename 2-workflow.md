#### типичный процесс работы с торчом
1. подготовка данных (60-80% данных на обучение, 10-20 на валидацию, остальное 10-20 тест)
2. построение модели ()
3. обучение на данных
4. предсказание (предикт) - ```torch.inference_mode()``` тоже самое что ```torch.no_grad()``` (```no_grad()``` в старом торче)
    - inference_mode() - вырубает градиентное отслеживание парамтеров, для инференсе эту не нужно (для обучения да), короче можно быстро увидеть предикт
```python
with torch.inference_mode(): 
    y_preds = model_0(X_test)
```
5. сохранить модель

#### 1 подготовка данных
- ну тут все тривиально train, test делаем фреймы

#### 2 построение модели
- в торче 4 параметра с которыми можно создать нейронку
- ```torch.nn``` 
  - блоки для строительнства вычисления графов
- ```torch.nn.Parameter```
  - состоит из тензоров их можно юзать с ```nn.Module```, есть параметр ```requires_grad=True``` чтобы обновлять параметры модели с помощзью градиентного спуска
  - в нем веса  или смещение
- ```torch.nn.Module```
  - базовый класс для всех нейронок, все куски нейронки это эти подклассы, если нейронка на торче то она состоит из ```nn.Module``` и у нее есть метод ```forward()```
  - содержит большие блоки = слои
- ```torch.optim``` 
  - в нем разные оптимизационные алгоритмы, помогает параметр в ```nn.Parameter``` чтобы улучшить градиентный спуск
- ```def forward()```	
  - все подклассы от nn.Module должны иметь этот метод, определяет порядок расчета 

#### 3 обучение модели на данных
- большую часть времени ты не будешь знать идеальные параметры для модели :)
- функция потерь
  - функция потерь показывает как хорошо модель предсказывает реальное значение, и чтобы норм модель была надо эту функции потерь минимизировать
  - в торче много функций потерь, самые основные:
    - в торче они тут torch.nn.
    - средняя абсолютная ошибка - усреднённая сумма модулей разницы между реальным и предсказанным значениями (torch.nn.L1Loss()) - менее чувствительна к выбросам. 
    - кросс-энтропия (или логарифмическая функция потерь – log loss) - Кросс-энтропия измеряет расхождение между двумя вероятностными распределениями. Если кросс-энтропия велика, значит, что разница между двумя распределениями велика, если мала, то распределения похожи друг на друга (torch.nn.BCELoss()).
- оптимайзер
  - покажет как обновить внутренние параметры моедил, чтобы минимизровать функцию потерь
  - оптимайзеры тут torch.optim основные:
    - стохастический градиентный спуск - на каждой итерации алгоритма из обучающей выборки каким-то (случайным) образом выбирается только один объект
    - Adam - это алгоритм оптимизации замены для стохастического градиентного спуска, может обрабатывать редкие градиенты в шумных задачах