### машин ленинг - мл
### глубокое обучение - го

#### для чего не подходит го?
- когда нужно пояснение обучения (паттерны го обычно для чела нечитабельны)
- когда классик традишнл методы лучше
- когда ошибки неприемлемы (выход моделей го не всегда предсказуем)
- когда мало входных данных

#### отличие мл от го
- в **мл** входные данные структурированы (типа табличка) - для этого кейса лучший алгоритм **gradient boosted machine** (xgboost)
- в **го** лучше неструктурированные данные (из разных признаков делать выводы) - для этого кейса **нейронки**


#### что такое нейронка (нейронная сеть)?
- вообще с чего начинается обучение?
  - входные данные (картинки, текст, аудио, итд) -> в массив чисел -> в обучающую нейронку (подходящую для кейса) -> на выходе массив чисел
- нейронка состоит из слоев
  - первый слой для входных данных
  - второй слой - скрытые слои для обучения паттернов(весов, эмбеддингов, фичи) на данных
  - третий слой выходные данные
  - <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/250px-Colored_neural_network.svg.png" width="130" height="150">

#### типы обучений
- обучение с учителем (есть данные и ответы которым должны соотвествовать результаты)
- обучение без учителя (есть данные но нет ответов, и выделям паттерны которые позволяют отличить данные, типа собаку от кошки)
- обучение с переносом (опыт и паттерны одной нейронки могут быть применимы в обучении другой нейронки)

#### когда применимо го?
- рекомендации кино, товары итд ()
- перевод языков (sequence2sequence)
- распознование голоса (sequence2sequence)
- распознование картинок (регрессия)
- нлп (классификация)

#### что такое pytorch (далее торч)? (ЧТО ТЫ ТАКОЕ????)
- самый популярный фреймворк для го
- может работать на гпу

#### что такое тензор? это фундаментальные блоки для торча
- числовой массив, числовое представление (картинку перевели в числа или выход нейронки)
- мат операции в нейронке ведутся над тензором

#### что такое скаляр?
- тензор с размерностью ноль (это одно число)

#### что такое вектор?
- это одноразмерный тензор (уже может содержать числа)
```python 
vector = torch.tensor([9, 10])
vector  # tensor([7, 7])
vector.ndim  # 1 размер равен 1
```
- вектором уже можно описать что-то, типа [кот, собака] это [9, 10] в числовом представлении 
#### как узнать размерность тензора?
- размерность можно узнать посчитав квадратные скобки с одной стороны tensor([7, 7]) - здесь размерность = 1
- пример ```torch.tensor([[7, 8], [9, 10]])``` тут = 2

#### что такое матрица?
- массив с размерностью = 2

<img src="https://forum.huawei.com/enterprise/en/data/attachment/forum/202211/07/191941v6w0x9ljxjljrifb.png" width="400" height="150">

#### нулевые тензоры
- когда нужно создать "маску" из нулей и применить ее на оригинальном тензоре, чтобы не учесть данные которые подпадают под нули
```python
zeros = torch.zeros(size=(2, 3))
# (tensor([[0., 0., 0.],
#         [0., 0., 0.]])
```
- или с единицами torch.ones(...)

#### самый популярный тип данных в торче + задать тип девайса (GPU, CPU)
- torch.float32 или torch.float
- создать тензор с типом данных: ```torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32, device=None,)```
- чем меньше разряд, тем меньше точность, но быстрее, пример в нейронках мобилок может ```torch.int8```, так как быстрее

#### операции над тензором
- сложение ```torch.tensor([1, 2, 3]) + 10 = tensor([11, 12, 13])``` или torch.add
- умножение ```torch.tensor([1, 2, 3]) * 10 = tensor([10, 20, 30])``` или torch.multiply
- вычитание ```torch.tensor([1, 2, 3]) - 10 = tensor([-9, -8, -7])```
- но обычно знаками, а не фичами торча

#### в го и мл самая популярная операции над матрицами - умножение матриц
- torch.matmul()
- пример
```python
tensor = torch.tensor([1, 2, 3])
# tensor * tensor = [1*1, 2*2, 3*3] = tensor([1, 4, 9])
# torch.matmul(tensor, tensor) = [1*1 + 2*2 + 3*3] = tensor(14)
```

#### самая частая ошибка в переумножении матриц - несовпадение размерностей
- можно решить если одну из матриц транспонировать например (матрица В после транспонирования)
<img src="https://devpractice.ru/wp-content/uploads/2019/04/linal-lesson3-pic23.png" width="270" height="150">
- можно визуально глянуть как происходит умножение: http://matrixmultiplication.xyz/

#### нейронки полны этими умножениями матриц
- нейронка с прямой связью (соединения между узлами не обрузуют цикл, инфа в одном направлении идет)
  - torch.nn.Linear() - воспроизводит матричное умножение между входным х и весами матрицы А
    - y = x*A^t + b (линейная функция)
    - x - входные данные для слоя, А - веса, b - смещение для весов и входных данных
    - y - это выход

#### назождение min, max, mean, и их позицй
- ```x = tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])```
  - ```torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)```
  - позиции: ```tensor.argmax(), tensor.argmin()```

#### еще операции надо тензором
- reshape - поменять размерность
- stack - уложить этот, или несколько тензоров в один (как стэковая укладка)
- squeeze - удалить размерность
- unsqueeze - добавить размерность
- permute - оси в тензоре поменять местами

#### также можно по индексу дергать значения
```python
x = tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
x[:, 0] вернет tensor([[1, 2, 3]])
```

#### работа торча на гпу (видеокарта)
- установка девайса для расчетов
  - ```device = "cuda" if torch.cuda.is_available() else "cpu"```
- кол-во гпу: ```torch.cuda.device_count()```
- можно "положить" тензор на гпу
```python
tensor = torch.tensor([1, 2, 3])
# закинем на гпу если есть
tensor_on_gpu = tensor.to(device)
```

#### вернем тензор обртано на цпу
- tensor_back_on_cpu = tensor_on_gpu.cpu().numpy() 
  - верент копию тензора в tensor_back_on_cpu

####